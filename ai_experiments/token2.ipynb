{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c56e9ba7-8980-40f1-a7ef-8b31cd263d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "import logging\n",
    "import csv\n",
    "import os\n",
    "from enum import Enum\n",
    "import string\n",
    "import time\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s %(asctime)s [%(filename)s:%(lineno)d]: %(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "132e31ad-f395-4f48-b850-3f9dc8b2fee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/almaz/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/almaz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d1dff43-ec4c-4055-8fd2-0b1fe26d72bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(\n",
    "        max_iter=1900  # 1900\n",
    "    )\n",
    "model = RandomForestRegressor() # 0.72\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=60, \n",
    "    oob_score=True, \n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=2,\n",
    "    # max_features='sqrt',\n",
    "    criterion=\"friedman_mse\"\n",
    "    # max_depth=12\n",
    ")\n",
    "vectorizer = TfidfVectorizer(stop_words=list(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f7ca896-aaca-454d-8493-4148013f50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_symbols = (\n",
    "        string.ascii_lowercase +\n",
    "        \" -+%\" +\n",
    "        # string.digits +\n",
    "        \"абвгдежзийклмнопрстуфхцчшщъыьэюя\"\n",
    ")\n",
    "def getClearText(text: str) -> str:\n",
    "    cleared_text = \"\"\n",
    "    for letter in text:\n",
    "        if letter in allowed_symbols:\n",
    "            cleared_text += letter\n",
    "\n",
    "    return cleared_text\n",
    "\n",
    "replace_dict = {\n",
    "    \" +\": \" плюс \",\n",
    "    \" -\": \" минус \",\n",
    "    \"%\": \" процент \"\n",
    "}\n",
    "def replaceSymbols(text: str) -> str:\n",
    "    new_text = text\n",
    "    for key, value in replace_dict.items():\n",
    "        new_text = new_text.replace(key, value)\n",
    "    return new_text\n",
    "\n",
    "def getFullClearText(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    cleared_text = getClearText(text)\n",
    "    cleared_text = replaceSymbols(cleared_text)\n",
    "    return cleared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8780e527-4324-460e-b89b-8e8fa1ea8f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(db_path: str, boundary: float = None):\n",
    "    global model, vectorizer\n",
    "\n",
    "    df = pd.read_csv(db_path)\n",
    "    df_texts = df.Text\n",
    "\n",
    "    if boundary is None:\n",
    "        boundary = (df.Value.max() + df.Value.min()) / 2\n",
    "        boundary = df.Value.median()\n",
    "        print(\"Boundary value set to \", boundary)\n",
    "    \n",
    "    # values = np.array([-1 if value < boundary else 1 for value in df.Value])\n",
    "\n",
    "    cleared = []\n",
    "    for text in df_texts:\n",
    "        cleared.append(getFullClearText(text))\n",
    "        # print(cleared[-1])\n",
    "        # break\n",
    "        # print(cleared[-1])\n",
    "        # print(\"-----\")\n",
    "        \n",
    "\n",
    "    vectorized = vectorizer.fit_transform(cleared)\n",
    "\n",
    "    # for i in range(50):\n",
    "    #     df_vectorized = pd.DataFrame(vectorized[i].T.todense(),\n",
    "    #                   index=vectorizer.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "    #     df_vectorized = df_vectorized.sort_values('TF-IDF', ascending=False)\n",
    "    #     print(df_vectorized.head(10))\n",
    "\n",
    "    test_size = 0.2\n",
    "    random_state = 134134\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        vectorized,\n",
    "        df.Value,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # so we can get original text but not tokenized\n",
    "    x_original_train, x_original_test, _, _ = train_test_split(\n",
    "        df_texts,\n",
    "        df.Value,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # with open('model_rf60.pkl', 'rb') as f:\n",
    "    #     model = pickle.load(f)\n",
    "    #     logging.info(\"Model loaded\")\n",
    "\n",
    "    # x_test_texts = vectorizer.inverse_transform(x_test)\n",
    "    # print(x_test)\n",
    "    # print(x_test_texts)\n",
    "\n",
    "    if \"model_rf.pkl\" not in os.listdir(\".\"):\n",
    "        logging.info(\"Model fitting process started\")\n",
    "\n",
    "        model.fit(vectorized, df.Value)\n",
    "\n",
    "        logging.info(\"Model fit\")\n",
    "\n",
    "        with open('model_rf.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "            logging.info(\"Model saved\")\n",
    "\n",
    "    else:\n",
    "        logging.info(\"Loading model...\")\n",
    "        with open('model_rf.pkl', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "            logging.info(\"Model loaded\")\n",
    "    \n",
    "    # model.fit(vectorized, df.Value)\n",
    "\n",
    "    # with open('model_rf300.pkl', 'wb') as f:\n",
    "    #     pickle.dump(model, f)\n",
    "    #     print(\"Model saved\")\n",
    "\n",
    "    y_predict = model.predict(x_test)\n",
    "    print(\"model score:\", model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "208213f8-a11d-4ab6-8a54-8fe561461c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boundary value set to  0.0097296929921857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-01-15 18:23:02,651 [395965137.py:69]: Loading model...\n",
      "INFO 2025-01-15 18:23:02,664 [395965137.py:72]: Model loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.7260099349141271\n",
      "3.206568956375122 seconds passed\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "fit(\"database_new.csv\")\n",
    "print(time.time() - start, \"seconds passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "92154f8f-70ec-4fe3-9a2d-da6aa843a4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815\n"
     ]
    }
   ],
   "source": [
    "i = -0.45\n",
    "while i != 1:\n",
    "    print(i, end=\" \")\n",
    "    fit(\"database_new.csv\", i)\n",
    "    i += 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2103daa5-3340-442d-8710-f20a5409c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ticker: str) -> str:\n",
    "    global model\n",
    "\n",
    "    csv_path = f\"../parse_experiments/recent_news/{ticker}.csv\"\n",
    "    try:\n",
    "        with open(\n",
    "                csv_path,\n",
    "                mode='r',\n",
    "                newline='',\n",
    "                encoding='utf-8'\n",
    "        ) as file:\n",
    "            file.seek(0)\n",
    "            reader = csv.DictReader(file)\n",
    "            news = []\n",
    "            predictors = []\n",
    "\n",
    "            for i, row in enumerate(reader):\n",
    "                title = row['Title']\n",
    "                text = row['Text']\n",
    "                time = row['Time']\n",
    "                url = row['Url']\n",
    "\n",
    "                time = datetime.datetime.strptime(time, \"%d.%m.%Y, %H:%M\")\n",
    "                if time < datetime.datetime.now() - datetime.timedelta(days=365 * 2):\n",
    "                    continue\n",
    "\n",
    "                news.append({})\n",
    "                news[i][\"text\"] = row[\"Text\"]\n",
    "                news[i][\"url\"] = row[\"Url\"]\n",
    "                news[i][\"title\"] = row[\"Title\"]\n",
    "                news[i][\"text_cleared\"] = getFullClearText(text)\n",
    "\n",
    "                predictors.append(news[i][\"text_cleared\"])\n",
    "\n",
    "            if len(news) == 0:\n",
    "                return f\"Не нашлось новостей за последние два года для {ticker} :(\"\n",
    "\n",
    "            vectorized = vectorizer.transform(predictors)\n",
    "\n",
    "            y_predict = model.predict(vectorized)\n",
    "\n",
    "            logging.info(f\"PREDICT {y_predict}\")\n",
    "\n",
    "            verdict = f\"Оценка новостей связанных с компанией {ticker.upper()}:\\n\"\n",
    "\n",
    "            perspectivity = 0\n",
    "            for i in range(len(news)):\n",
    "                if y_predict[i] < -0.02:\n",
    "                    mark = \"❌\"\n",
    "                    perspectivity -= 1\n",
    "                else:\n",
    "                    mark = \"✅\"\n",
    "                    perspectivity += 1\n",
    "\n",
    "                sample = f\"{mark} {i + 1}. [{news[i]['title']}]({news[i]['url']})\"\n",
    "                verdict += sample + \"\\n\"\n",
    "\n",
    "            overall_mark = \"НЕПЕРСПЕКТИВНАЯ\" if perspectivity < 0 else \"ПЕРСПЕКТИВНАЯ\"\n",
    "            verdict += (\"\\n\"\n",
    "                        f\"Общая оценка: *{overall_mark}*\")\n",
    "\n",
    "            return verdict\n",
    "\n",
    "    except FileNotFoundError as ex:\n",
    "        return f\"Не нашлось новостей за последние два года для {ticker} :(\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04afcd61-d5af-47ac-869f-6f1002d81abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-01-15 18:33:41,624 [1501961162.py:42]: PREDICT [-0.03438189  0.05092991 -0.01463035  0.01891648]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Оценка новостей связанных с компанией POSI:\\n❌ 1. [Акции «Группы Позитив» за два дня потеряли более 7% на новостях о допэмиссии](https://ru.investing.com/news/stock-market-news/article-2299269)\\n✅ 2. [Презентация шлема смешанной реальности от Apple: новости к утру 6 июня](https://ru.investing.com/news/economy/article-2261668)\\n✅ 3. [Чистая прибыль «Группы Позитив» в 2022 году выросла более чем в три раза](https://ru.investing.com/news/stock-market-news/article-2247152)\\n✅ 4. [Positive Technologies намерена существенно увеличить дивиденды](https://ru.investing.com/news/stock-market-news/article-2239788)\\n\\nОбщая оценка: *ПЕРСПЕКТИВНАЯ*'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"POSI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a66fd24-3b5f-430f-ac27-243a770e8056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefbef2d-72fc-411c-9a54-3c60aa8ffade",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
