{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c56e9ba7-8980-40f1-a7ef-8b31cd263d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pickle\n",
    "import logging\n",
    "import csv\n",
    "import os\n",
    "from enum import Enum\n",
    "import string\n",
    "import time\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(levelname)s %(asctime)s [%(filename)s:%(lineno)d]: %(message)s\",\n",
    "    level=logging.DEBUG,\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "132e31ad-f395-4f48-b850-3f9dc8b2fee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/almaz/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/almaz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0d1dff43-ec4c-4055-8fd2-0b1fe26d72bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(\n",
    "        max_iter=1900  # 1900\n",
    "    )\n",
    "model = RandomForestRegressor() # 0.72\n",
    "model = RandomForestRegressor(n_estimators=120)\n",
    "vectorizer = TfidfVectorizer(stop_words=list(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ca896-aaca-454d-8493-4148013f50bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_symbols = (\n",
    "        string.ascii_lowercase +\n",
    "        \" -+%\" +\n",
    "        # string.digits +\n",
    "        \"Ð°Ð±Ð²Ð³Ð´ÐµÐ¶Ð·Ð¸Ð¹ÐºÐ»Ð¼Ð½Ð¾Ð¿Ñ€ÑÑ‚ÑƒÑ„Ñ…Ñ†Ñ‡ÑˆÑ‰ÑŠÑ‹ÑŒÑÑŽÑ\"\n",
    ")\n",
    "def getClearText(text: str) -> str:\n",
    "    cleared_text = \"\"\n",
    "    for letter in text:\n",
    "        if letter in allowed_symbols:\n",
    "            cleared_text += letter\n",
    "\n",
    "    return cleared_text\n",
    "\n",
    "replace_dict = {\n",
    "    \" +\": \" Ð¿Ð»ÑŽÑ \",\n",
    "    \" -\": \" Ð¼Ð¸Ð½ÑƒÑ \",\n",
    "    \"%\": \" Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚ \"\n",
    "}\n",
    "def replaceSymbols(text: str) -> str:\n",
    "    new_text = text\n",
    "    for key, value in replace_dict.items():\n",
    "        new_text = new_text.replace(key, value)\n",
    "    return new_text\n",
    "\n",
    "def getFullClearText(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    cleared_text = getClearText(text)\n",
    "    cleared_text = replaceSymbols(cleared_text)\n",
    "    return cleared_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8780e527-4324-460e-b89b-8e8fa1ea8f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(db_path: str, boundary: float = None):\n",
    "    global model, vectorizer\n",
    "\n",
    "    df = pd.read_csv(db_path)\n",
    "    df_texts = df.Text\n",
    "\n",
    "    if boundary is None:\n",
    "        boundary = (df.Value.max() + df.Value.min()) / 2\n",
    "        boundary = df.Value.median()\n",
    "        print(\"Boundary value set to \", boundary)\n",
    "    \n",
    "    # values = np.array([-1 if value < boundary else 1 for value in df.Value])\n",
    "\n",
    "    cleared = []\n",
    "    for text in df_texts:\n",
    "        cleared.append(getFullClearText(text))\n",
    "        # print(cleared[-1])\n",
    "        # break\n",
    "        # print(cleared[-1])\n",
    "        # print(\"-----\")\n",
    "        \n",
    "\n",
    "    vectorized = vectorizer.fit_transform(cleared)\n",
    "\n",
    "    # for i in range(50):\n",
    "    #     df_vectorized = pd.DataFrame(vectorized[i].T.todense(),\n",
    "    #                   index=vectorizer.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "    #     df_vectorized = df_vectorized.sort_values('TF-IDF', ascending=False)\n",
    "    #     print(df_vectorized.head(10))\n",
    "\n",
    "    # test_size = 0\n",
    "    # random_state = 8471874\n",
    "    \n",
    "    # x_train, x_test, y_train, y_test = train_test_split(\n",
    "    #     vectorized,\n",
    "    #     df.Value,\n",
    "    #     test_size=test_size,\n",
    "    #     random_state=random_state\n",
    "    # )\n",
    "    \n",
    "    # # so we can get original text but not tokenized\n",
    "    # x_original_train, x_original_test, _, _ = train_test_split(\n",
    "    #     df_texts,\n",
    "    #     df.Value,\n",
    "    #     test_size=test_size,\n",
    "    #     random_state=random_state\n",
    "    # )\n",
    "\n",
    "    # x_test_texts = vectorizer.inverse_transform(x_test)\n",
    "    # print(x_test)\n",
    "    # print(x_test_texts)\n",
    "\n",
    "    if \"model_rf120.pkl\" not in os.listdir(\".\"):\n",
    "        logging.info(\"Model fitting process started\")\n",
    "\n",
    "        model.fit(vectorized, df.Value)\n",
    "\n",
    "        logging.info(\"Model fit\")\n",
    "\n",
    "        with open('model_rf120.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "            logging.info(\"Model saved\")\n",
    "\n",
    "    else:\n",
    "        logging.info(\"Loading model...\")\n",
    "        with open('model_rf120.pkl', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "            logging.info(\"Model loaded\")\n",
    "    \n",
    "    # model.fit(vectorized, df.Value)\n",
    "\n",
    "    # with open('model_rf300.pkl', 'wb') as f:\n",
    "    #     pickle.dump(model, f)\n",
    "    #     print(\"Model saved\")\n",
    "\n",
    "    # y_predict = model.predict(x_test)\n",
    "    # print(model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "208213f8-a11d-4ab6-8a54-8fe561461c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boundary value set to  0.0097296929921857\n",
      "Model saved\n",
      "781.1506631374359 seconds passed\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "fit(\"database_new.csv\")\n",
    "print(time.time() - start, \"seconds passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "92154f8f-70ec-4fe3-9a2d-da6aa843a4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815\n"
     ]
    }
   ],
   "source": [
    "i = -0.45\n",
    "while i != 1:\n",
    "    print(i, end=\" \")\n",
    "    fit(\"database_new.csv\", i)\n",
    "    i += 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2103daa5-3340-442d-8710-f20a5409c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ticker: str) -> str:\n",
    "    csv_path = f\"../parse_experiments/recent_news/{ticker}.csv\"\n",
    "    try:\n",
    "        with open(\n",
    "                csv_path,\n",
    "                mode='r',\n",
    "                newline='',\n",
    "                encoding='utf-8'\n",
    "        ) as file:\n",
    "            file.seek(0)\n",
    "            reader = csv.DictReader(file)\n",
    "            news = []\n",
    "            predictors = []\n",
    "\n",
    "            for i, row in enumerate(reader):\n",
    "                title = row['Title']\n",
    "                text = row['Text']\n",
    "                time = row['Time']\n",
    "                url = row['Url']\n",
    "\n",
    "                time = datetime.datetime.strptime(time, \"%d.%m.%Y, %H:%M\")\n",
    "                if time < datetime.datetime.now() - datetime.timedelta(days=365 * 2):\n",
    "                    continue\n",
    "\n",
    "                news.append({})\n",
    "                news[i][\"text\"] = row[\"Text\"]\n",
    "                news[i][\"url\"] = row[\"Url\"]\n",
    "                news[i][\"title\"] = row[\"Title\"]\n",
    "                news[i][\"text_cleared\"] = getFullClearText(text)\n",
    "\n",
    "                predictors.append(news[i][\"text_cleared\"])\n",
    "\n",
    "            if len(news) == 0:\n",
    "                return f\"ÐÐµ Ð½Ð°ÑˆÐ»Ð¾ÑÑŒ Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹ Ð·Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ Ð´Ð²Ð° Ð³Ð¾Ð´Ð° Ð´Ð»Ñ {ticker} :(\"\n",
    "\n",
    "            vectorized = vectorizer.transform(predictors)\n",
    "\n",
    "            y_predict = model.predict(vectorized)\n",
    "\n",
    "            logging.info(f\"PREDICT {y_predict}\")\n",
    "\n",
    "            verdict = f\"ÐžÑ†ÐµÐ½ÐºÐ° Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹ ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ñ… Ñ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸ÐµÐ¹ {ticker.upper()}:\\n\"\n",
    "\n",
    "            for i in range(len(news)):\n",
    "                if y_predict[i] < 0:\n",
    "                    mark = \"ðŸ’©\"\n",
    "                else:\n",
    "                    mark = \"ðŸ¤‘\"\n",
    "                    \n",
    "                sample = f\"{mark} {i + 1}. [{news[i]['title']}]({news[i]['url']})\"\n",
    "                verdict += sample + \"\\n\"\n",
    "\n",
    "            overall_mark = \"ÐÐ•ÐŸÐ•Ð Ð¡ÐŸÐ•ÐšÐ¢Ð˜Ð’ÐÐÐ¯\" if sum(y_predict) < 0 else \"ÐŸÐ•Ð Ð¡ÐŸÐ•ÐšÐ¢Ð˜Ð’ÐÐÐ¯\"\n",
    "            verdict += (\"\\n\"\n",
    "                        f\"ÐžÐ±Ñ‰Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ°: *{overall_mark}*\")\n",
    "\n",
    "            return verdict\n",
    "\n",
    "    except FileNotFoundError as ex:\n",
    "        return f\"ÐÐµ Ð½Ð°ÑˆÐ»Ð¾ÑÑŒ Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹ Ð·Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ Ð´Ð²Ð° Ð³Ð¾Ð´Ð° Ð´Ð»Ñ {ticker} :(\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "04afcd61-d5af-47ac-869f-6f1002d81abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2025-01-15 12:19:20,028 [2595241935.py:40]: PREDICT [0.505]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ÐžÑ†ÐµÐ½ÐºÐ° Ð½Ð¾Ð²Ð¾ÑÑ‚ÐµÐ¹ ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ñ… Ñ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸ÐµÐ¹ DATA:\\nðŸ¤‘ 1. [Â«ÐÑ€ÐµÐ½Ð°Ð´Ð°Ñ‚Ð°Â» Ð¾Ð±ÑŠÑÐ²Ð¸Ð»Ð° Ð´Ð¸Ð°Ð¿Ð°Ð·Ð¾Ð½ Ñ†ÐµÐ½ Ð´Ð»Ñ IPO Ð½Ð° ÐœÐ¾ÑÐ±Ð¸Ñ€Ð¶Ðµ](https://ru.investing.com/news/stock-market-news/article-2513189)\\n\\nÐžÐ±Ñ‰Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ°: *ÐŸÐ•Ð Ð¡ÐŸÐ•ÐšÐ¢Ð˜Ð’ÐÐÐ¯*'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a66fd24-3b5f-430f-ac27-243a770e8056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
